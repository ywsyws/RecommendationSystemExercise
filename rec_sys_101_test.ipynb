{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "ratings = pd.read_csv(\n",
    "    'data/ml-100k/u.data', sep='\\t', names=ratings_cols, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users = 943 | Number of movies = 1682\n"
     ]
    }
   ],
   "source": [
    "n_users = ratings.user_id.unique().shape[0]\n",
    "n_movies = ratings.movie_id.unique().shape[0]\n",
    "print('Number of users = ' + str(n_users) + ' | Number of movies = ' + str(n_movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(ratings, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>unix_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49447</th>\n",
       "      <td>488</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>891293974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35988</th>\n",
       "      <td>295</td>\n",
       "      <td>427</td>\n",
       "      <td>4</td>\n",
       "      <td>879517412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55917</th>\n",
       "      <td>593</td>\n",
       "      <td>385</td>\n",
       "      <td>4</td>\n",
       "      <td>886194041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25044</th>\n",
       "      <td>130</td>\n",
       "      <td>246</td>\n",
       "      <td>4</td>\n",
       "      <td>874953698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57304</th>\n",
       "      <td>798</td>\n",
       "      <td>365</td>\n",
       "      <td>3</td>\n",
       "      <td>875639656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  movie_id  rating  unix_timestamp\n",
       "49447      488        50       4       891293974\n",
       "35988      295       427       4       879517412\n",
       "55917      593       385       4       886194041\n",
       "25044      130       246       4       874953698\n",
       "57304      798       365       3       875639656"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create User-Item Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two user-item matrices, one for training and another for testing\n",
    "train_data_UI_matrix = np.zeros((n_users, n_movies))\n",
    "for line in train_data.itertuples():\n",
    "    train_data_UI_matrix[line[1]-1, line[2]-1] = line[3]\n",
    "    \n",
    "test_data_UI_matrix = np.zeros((n_users, n_movies))\n",
    "for line in test_data.itertuples():\n",
    "    test_data_UI_matrix[line[1]-1, line[2]-1] = line[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memeory-Based CF by Computing Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Cosine Similarity\n",
    "Note, the output will range from 0 to 1 since the ratings are all positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 943)\n",
      "(1682, 1682)\n"
     ]
    }
   ],
   "source": [
    "user_similarity = pairwise_distances(train_data_UI_matrix, metric='cosine')\n",
    "item_similarity = pairwise_distances(train_data_UI_matrix.T, metric='cosine')\n",
    "print(user_similarity.shape)\n",
    "print(item_similarity.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.88924119, 0.97547817, ..., 0.91464761, 0.85205611,\n",
       "        0.68931606],\n",
       "       [0.88924119, 0.        , 0.85084855, ..., 0.82651205, 0.85730583,\n",
       "        0.92076344],\n",
       "       [0.97547817, 0.85084855, 0.        , ..., 0.92489063, 0.87113324,\n",
       "        0.97955186],\n",
       "       ...,\n",
       "       [0.91464761, 0.82651205, 0.92489063, ..., 0.        , 0.89592051,\n",
       "        0.97144285],\n",
       "       [0.85205611, 0.85730583, 0.87113324, ..., 0.89592051, 0.        ,\n",
       "        0.89657686],\n",
       "       [0.68931606, 0.92076344, 0.97955186, ..., 0.97144285, 0.89657686,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(ratings, similarity, type=\"user\"):\n",
    "    if type == \"user\":\n",
    "        mean_user_rating = ratings.mean(axis=1)\n",
    "        # you use np.newaxis so that mean_user_rating has same format as ratings\n",
    "        rating_diff = ratings - mean_user_rating[:, np.newaxis]\n",
    "#         print((mean_user_rating[:, np.newaxis]).shape)\n",
    "#         print(similarity.shape)\n",
    "#         print(rating_diff.shape)\n",
    "#         print((similarity.dot(rating_diff)).shape)\n",
    "#         print((np.array([np.abs(similarity).sum(axis=0)])).shape)\n",
    "#         print((np.array([np.abs(similarity).sum(axis=1)])).shape)\n",
    "        pred = (\n",
    "            mean_user_rating[:, np.newaxis]\n",
    "            + similarity.dot(rating_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "        )\n",
    "#         print(pred.shape)\n",
    "    elif type == \"item\":\n",
    "#         print(ratings.shape)\n",
    "#         print(similarity.shape)\n",
    "#         print((np.array([np.abs(similarity).sum(axis=0)])).shape)\n",
    "        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n",
    "#         print(pred.shape)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 1682)\n",
      "(943, 1682)\n"
     ]
    }
   ],
   "source": [
    "item_prediction = predict(train_data_UI_matrix, item_similarity, type='item')\n",
    "user_prediction = predict(train_data_UI_matrix, user_similarity, type='user')\n",
    "print(item_prediction.shape)\n",
    "print(user_prediction.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Using the most popular metric: RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(prediction, ground_truth):\n",
    "#     print(prediction.shape)\n",
    "#     print(ground_truth.shape)\n",
    "#     print(ground_truth.nonzero())\n",
    "#     print(prediction[ground_truth.nonzero()])\n",
    "#     print(prediction[ground_truth.nonzero()].flatten())\n",
    "#     print(ground_truth[ground_truth.nonzero()].flatten())\n",
    "    prediction = prediction[ground_truth.nonzero()]#.flatten()\n",
    "    ground_truth = ground_truth[ground_truth.nonzero()]#.flatten()\n",
    "    return round(sqrt(mean_squared_error(prediction, ground_truth)), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-base CF RMSE: 3.122\n",
      "Item-base CF RMSE: 3.4476\n"
     ]
    }
   ],
   "source": [
    "print('User-base CF RMSE: ' + str(rmse(user_prediction, test_data_UI_matrix)))\n",
    "print('Item-base CF RMSE: ' + str(rmse(item_prediction, test_data_UI_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based CF by Using Sigular Value Decomposition (SVD)\n",
    "based on matrix factorization (MF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Matrix Factorization(MF)](https://developers.google.com/machine-learning/recommendation/images/Matrixfactor.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/91/0*qZBT80xpE9OYAuEr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given `m x n` matrix X:<br>\n",
    "* `U` is an `(m x r)` orthogonal matrix\n",
    "* `S` is an `(r x r)` diagonal matrix with non-negative real numbers on the diagonal\n",
    "* `V^T` is an `(r x n)` orthogonal matrix <br>\n",
    "Elements on the diagonal in `S` are known as *singular values* of `X`.<br>\n",
    "Matrix `X` can be factorized to `U`, `S` and `V`. The `U` matrix represents the feature vectors corresponding to the users in the hidden feature space and the `V` matrix represents the feature vectors corresponding to the items in the hidden feature space.<br>\n",
    "Now you can make a prediction by taking product of `U`, `S` and `V^T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sparsity level of MovieLen100K is 93.7%\n"
     ]
    }
   ],
   "source": [
    "sparsity = round(1.-len(ratings)/(n_users*n_movies), 3)\n",
    "print(f'The sparsity level of MovieLen100K is {str(sparsity*100)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SVD components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SVD components from the train matrix. Choose k.\n",
    "u, s, vt = svds(train_data_UI_matrix, k=20)\n",
    "s_diag_matrix = np.diag(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = np.dot(np.dot(u, s_diag_matrix), vt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Using the most popular metric: RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-base CF RMSE: 2.7185\n"
     ]
    }
   ],
   "source": [
    "print('User-base CF RMSE: ' + str(rmse(X_pred, test_data_UI_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>unix_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating  unix_timestamp\n",
       "0      196       242       3       881250949\n",
       "1      186       302       3       891717742\n",
       "2       22       377       1       878887116\n",
       "3      244        51       2       880606923\n",
       "4      166       346       1       886397596"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Collaborative Filtering (NCF) <br>  - Simple Version with Implicit Feeback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings['rank_latest'] = ratings.groupby(['user_id'])['unix_timestamp'].rank(method='first', ascending=False)\n",
    "train_ratings = ratings[ratings['rank_latest'] != 1]\n",
    "test_ratings = ratings[ratings['rank_latest'] == 1]\n",
    "\n",
    "train_ratings.drop(columns=['unix_timestamp', 'rank_latest'], inplace=True)\n",
    "test_ratings.drop(columns=['unix_timestamp', 'rank_latest'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60348</th>\n",
       "      <td>786</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98185</th>\n",
       "      <td>486</td>\n",
       "      <td>244</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46874</th>\n",
       "      <td>592</td>\n",
       "      <td>338</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50089</th>\n",
       "      <td>627</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33401</th>\n",
       "      <td>234</td>\n",
       "      <td>417</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  movie_id  rating\n",
       "60348      786         7       1\n",
       "98185      486       244       1\n",
       "46874      592       338       1\n",
       "50089      627        58       1\n",
       "33401      234       417       1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the dataset into an implicit feedback dataset\n",
    "train_ratings.loc[:, 'rating'] = 1\n",
    "train_ratings.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do have a problem now though. After binarizing our dataset, we see that every sample in the dataset now belongs to the positive class. However, we also require negative samples to train our models, to indicate movies that the user has not interacted with. We assume that such movies are those that the user are not interested in â€” even though this is a sweeping assumption that may not be true, it usually works out rather well in practice. <br>\n",
    "The code below generates 4 negative samples for each row of data. In other words, the ratio of negative to positive samples is 4:1. This ratio is chosen arbitrarily but I found that it works rather well in practice(feel free to find the best ratio yourself!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define PyTroch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensTrainDataset(Dataset):\n",
    "    \"\"\"MovieLens PyTorch Dataset for Training\n",
    "    \n",
    "    Args:\n",
    "        ratings (pd.DataFrame): Dataframe containing the movie ratings\n",
    "        all_movieIds (list): List containing all movieIds\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ratings, all_movieIds):\n",
    "        self.users, self.items, self.labels = self.get_dataset(ratings, all_movieIds)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.labels[idx]\n",
    "\n",
    "    def get_dataset(self, ratings, all_movieIds):\n",
    "        # Placeholders that will hold the training data\n",
    "        users, items, labels = [], [], []\n",
    "        # This is the set of items that each user has interaction with\n",
    "        user_item_set = set(zip(ratings['user_id'], ratings['movie_id']))\n",
    "        \n",
    "        # 4:1 ratio of negative to positive samples\n",
    "        num_negatives = 4\n",
    "        for u, i in user_item_set:\n",
    "            users.append(u)\n",
    "            items.append(i)\n",
    "            labels.append(1) # items that the user has interacted with are positive\n",
    "            for _ in range(num_negatives):\n",
    "                # randomly select an item\n",
    "                negative_item = np.random.choice(all_movieIds)\n",
    "                # check that the user has not interacted with this item\n",
    "                while (u, negative_item) in user_item_set:\n",
    "                    negative_item = np.random.choice(all_movieIds)\n",
    "                users.append(u)\n",
    "                items.append(negative_item)\n",
    "                labels.append(0) # items not interacted with are negative\n",
    "\n",
    "        return torch.tensor(users), torch.tensor(items), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define NCF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(pl.LightningModule):\n",
    "    \"\"\" Neural Collaborative Filtering (NCF)\n",
    "    \n",
    "        Args:\n",
    "            num_users (int): Number of unique users\n",
    "            num_items (int): Number of unique items\n",
    "            ratings (pd.DataFrame): Dataframe containing the movie ratings for training\n",
    "            all_movieIds (list): List containing all movieIds (train + test)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_users, num_items, ratings, all_movieIds):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(num_embeddings=num_users, embedding_dim=8)\n",
    "        self.item_embedding = nn.Embedding(num_embeddings=num_items, embedding_dim=8)\n",
    "        self.fc1 = nn.Linear(in_features=16, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.output = nn.Linear(in_features=32, out_features=1)\n",
    "        self.ratings = ratings\n",
    "        self.all_movieIds = all_movieIds\n",
    "        \n",
    "    def forward(self, user_input, item_input):\n",
    "        \n",
    "        # Pass through embedding layers\n",
    "        user_embedded = self.user_embedding(user_input)\n",
    "        item_embedded = self.item_embedding(item_input)\n",
    "\n",
    "        # Concat the two embedding layers\n",
    "        vector = torch.cat([user_embedded, item_embedded], dim=-1)\n",
    "        \n",
    "        # Pass through dense layer\n",
    "        vector = nn.ReLU()(self.fc1(vector))\n",
    "        vector = nn.ReLU()(self.fc2(vector))\n",
    "\n",
    "        # Output layer\n",
    "        pred = nn.Sigmoid()(self.output(vector))\n",
    "\n",
    "        return pred\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        user_input, item_input, labels = batch\n",
    "        predicted_labels = self(user_input, item_input)\n",
    "        loss = nn.BCELoss()(predicted_labels, labels.view(-1, 1).float())\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(MovieLensTrainDataset(self.ratings, self.all_movieIds),\n",
    "                          batch_size=512, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train NCF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.\n",
      "  f\"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will \"\n",
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=50)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:89: LightningDeprecationWarning: `reload_dataloaders_every_epoch` is deprecated in v1.4 and will be removed in v1.6. Please use `reload_dataloaders_every_n_epochs` in Trainer.\n",
      "  \"`reload_dataloaders_every_epoch` is deprecated in v1.4 and will be removed in v1.6.\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name           | Type      | Params\n",
      "---------------------------------------------\n",
      "0 | user_embedding | Embedding | 7.6 K \n",
      "1 | item_embedding | Embedding | 13.5 K\n",
      "2 | fc1            | Linear    | 1.1 K \n",
      "3 | fc2            | Linear    | 2.1 K \n",
      "4 | output         | Linear    | 33    \n",
      "---------------------------------------------\n",
      "24.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "24.2 K    Total params\n",
      "0.097     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726b0085454a4151b54eb65c379feac2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_users = ratings.user_id.nunique() + 1 # + bias term (?) (if RNN: to take into account the zero padding)\n",
    "num_items = ratings.movie_id.nunique() + 1 # + bias term (?) (if RNN: to take into account the zero padding)\n",
    "# Get a list of all movie IDs\n",
    "all_movieIds = ratings['movie_id'].unique()\n",
    "\n",
    "model = NCF(num_users, num_items, train_ratings, all_movieIds)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=5, gpus=0, reload_dataloaders_every_epoch=True,\n",
    "                     progress_bar_refresh_rate=50, logger=False, checkpoint_callback=False)\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Hit Ratio @ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Hit Ratio @ 10 is 0.41\n"
     ]
    }
   ],
   "source": [
    "# User-item pairs for testing\n",
    "test_user_item_set = set(zip(test_ratings['user_id'], test_ratings['movie_id']))\n",
    "\n",
    "# Dict of all items that are interacted with by each user\n",
    "user_interacted_items = ratings.groupby('user_id')['movie_id'].apply(list).to_dict()\n",
    "\n",
    "\n",
    "buy = []\n",
    "for (u,i) in test_user_item_set:\n",
    "    # For each user, randomly select 99 items that the user has not interacted with.\n",
    "    interacted_items = user_interacted_items[u]\n",
    "    not_interacted_items = set(all_movieIds) - set(interacted_items)\n",
    "    selected_not_interacted = list(np.random.choice(list(not_interacted_items), 99))\n",
    "    # Combine these 99 items with the test item (the actual item that \n",
    "    # the user last interacted with). We now have 100 items.\n",
    "    test_items = selected_not_interacted + [i]\n",
    "    \n",
    "    # Run the model on these 100 items\n",
    "    predicted_labels = np.squeeze(model(torch.tensor([u]*100), \n",
    "                                        torch.tensor(test_items)).detach().numpy())\n",
    "    # Select the top 10 items from the list of 100 items. \n",
    "    top10_items = [test_items[i] for i in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n",
    "    \n",
    "    # If the test item is present within the top 10 items, \n",
    "    # then we say that this is a hit.\n",
    "    if i in top10_items:\n",
    "        buy.append(1)\n",
    "    else:\n",
    "        buy.append(0)\n",
    "        \n",
    "print(\"The Hit Ratio @ 10 is {:.2f}\".format(np.average(buy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tqdm==4.40.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tqdm==4.40.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch-lightning==1.5.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow==2.6.2\r\n",
      "tensorflow-estimator==2.6.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze | grep tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288.55977869033813px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
